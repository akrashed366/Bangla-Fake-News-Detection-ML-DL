{"cells":[{"cell_type":"code","execution_count":1,"id":"68245265","metadata":{"id":"68245265","outputId":"723bb6ae-2f1d-4160-c299-c41cda2e6ef0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738064294041,"user_tz":-360,"elapsed":6917,"user":{"displayName":"Ak Rashed","userId":"12448226442715938476"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.17.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.21)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.69.0)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.17.1)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.5.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow) (0.14.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"]}],"source":["!pip install tensorflow"]},{"cell_type":"code","execution_count":2,"id":"86acffa8","metadata":{"id":"86acffa8","executionInfo":{"status":"ok","timestamp":1738064387993,"user_tz":-360,"elapsed":7437,"user":{"displayName":"Ak Rashed","userId":"12448226442715938476"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional,GRU\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","from tensorflow.keras.initializers import Constant\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YqdWjFOcuFzI","executionInfo":{"status":"ok","timestamp":1738064666821,"user_tz":-360,"elapsed":111891,"user":{"displayName":"Ak Rashed","userId":"12448226442715938476"}},"outputId":"8c825aad-a04b-4ccc-d5d8-ad222dec6341"},"id":"YqdWjFOcuFzI","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/My Drive/project/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yflPxQn3upXX","executionInfo":{"status":"ok","timestamp":1738064700712,"user_tz":-360,"elapsed":548,"user":{"displayName":"Ak Rashed","userId":"12448226442715938476"}},"outputId":"ba7c8abf-6d7c-407f-e88e-a27453be1a87"},"id":"yflPxQn3upXX","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/project\n"]}]},{"cell_type":"code","execution_count":7,"id":"88726825","metadata":{"id":"88726825","executionInfo":{"status":"ok","timestamp":1738064782152,"user_tz":-360,"elapsed":13768,"user":{"displayName":"Ak Rashed","userId":"12448226442715938476"}}},"outputs":[],"source":["true = pd.read_csv('Authentic-48K.csv', na_values=['#NAME?'])\n","false = pd.read_csv('Fake-1K.csv', na_values=['#NAME?'])"]},{"cell_type":"markdown","id":"b8597081","metadata":{"id":"b8597081"},"source":["# Word Embeddings(word2vec)"]},{"cell_type":"code","execution_count":8,"id":"2baf2121","metadata":{"id":"2baf2121","outputId":"753cf285-019e-46c7-9977-3444d5afcadf","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1738064792460,"user_tz":-360,"elapsed":805,"user":{"displayName":"Ak Rashed","userId":"12448226442715938476"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'data' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-ffb53c85f9fd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"]}],"source":["corpus=[]\n","words = 0;\n","j = 0\n","for i in data['content'].values:\n","    corpus.append(str(i).split(\" \"))\n","    words += len(corpus[j])\n","    j += 1\n","print(\"Total words = \", words)\n","print(\"Total Sentances = \", len(corpus))"]},{"source":["true = pd.read_csv('Authentic-48K.csv', na_values=['#NAME?'])\n","false = pd.read_csv('Fake-1K.csv', na_values=['#NAME?'])\n","\n","# Concatenate the true and false DataFrames into a single DataFrame called 'data'\n","data = pd.concat([true, false], ignore_index=True)\n","\n","corpus=[]\n","words = 0\n","j = 0\n","for i in data['content'].values:\n","    corpus.append(str(i).split(\" \"))\n","    words += len(corpus[j])\n","    j += 1\n","print(\"Total words = \", words)\n","print(\"Total Sentances = \", len(corpus))"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2U_3ujV2vK1X","executionInfo":{"status":"ok","timestamp":1738064849985,"user_tz":-360,"elapsed":12773,"user":{"displayName":"Ak Rashed","userId":"12448226442715938476"}},"outputId":"38f31422-01d2-488e-b8f1-abeaa698a1f3"},"id":"2U_3ujV2vK1X","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Total words =  13873886\n","Total Sentances =  49977\n"]}]},{"cell_type":"code","execution_count":10,"id":"ce5a612c","metadata":{"id":"ce5a612c","outputId":"7300da5a-8f4f-4e56-8fee-094dfd75116c","colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"status":"error","timestamp":1738064854918,"user_tz":-360,"elapsed":666,"user":{"displayName":"Ak Rashed","userId":"12448226442715938476"}}},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'bnlp'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-c276d8bb2de3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbnlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBengaliWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBengaliWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bnlp'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from bnlp import BengaliWord2Vec\n","import gensim\n","from gensim.models import Word2Vec\n","\n","bwv = BengaliWord2Vec()\n","\n","trained_model_path = \"./word_embeddings/bnwiki_word2vec.model\"\n","data_file = corpus\n","model_name = \"./word_embeddings/new_w2v.model\"\n","vector_name = \"./word_embeddings/new_w2v_vector.vector\"\n","bwv.pretrain(trained_model_path, data_file, model_name, vector_name, epochs=10)\n","\n","\n","w2v_model = Word2Vec.load(\"./word_embeddings/new_w2v.model\")\n","w2v_model.train(corpus,total_words=391114, epochs=10)\n","\n","w2v_model.wv.most_similar('নষ্ট')\n","w2v_model.wv['বাংলাদেশ'].shape"]},{"cell_type":"code","source":["!pip install bnlp_toolkit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YHTgJE3avZmP","executionInfo":{"status":"ok","timestamp":1738064923554,"user_tz":-360,"elapsed":22567,"user":{"displayName":"Ak Rashed","userId":"12448226442715938476"}},"outputId":"e178422b-cd74-4e94-d435-44be6079a009"},"id":"YHTgJE3avZmP","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bnlp_toolkit\n","  Downloading bnlp_toolkit-4.0.3-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.11/dist-packages (from bnlp_toolkit) (0.2.0)\n","Collecting gensim==4.3.2 (from bnlp_toolkit)\n","  Downloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from bnlp_toolkit) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bnlp_toolkit) (1.26.4)\n","Collecting scipy==1.10.1 (from bnlp_toolkit)\n","  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sklearn-crfsuite==0.3.6 (from bnlp_toolkit)\n","  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl.metadata (3.8 kB)\n","Collecting tqdm==4.66.3 (from bnlp_toolkit)\n","  Downloading tqdm-4.66.3-py3-none-any.whl.metadata (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ftfy==6.2.0 (from bnlp_toolkit)\n","  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\n","Collecting emoji==1.7.0 (from bnlp_toolkit)\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bnlp_toolkit) (2.32.3)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.11/dist-packages (from ftfy==6.2.0->bnlp_toolkit) (0.2.13)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2->bnlp_toolkit) (7.1.0)\n","Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite==0.3.6->bnlp_toolkit)\n","  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite==0.3.6->bnlp_toolkit) (1.17.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite==0.3.6->bnlp_toolkit) (0.9.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->bnlp_toolkit) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->bnlp_toolkit) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->bnlp_toolkit) (2024.11.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bnlp_toolkit) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bnlp_toolkit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bnlp_toolkit) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bnlp_toolkit) (2024.12.14)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.2->bnlp_toolkit) (1.17.2)\n","Downloading bnlp_toolkit-4.0.3-py3-none-any.whl (22 kB)\n","Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n","Downloading tqdm-4.66.3-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=1ec48be6f5dc1876f5dc83a3ce09290ff4715749d6c3c490c3e6c4f48d755b96\n","  Stored in directory: /root/.cache/pip/wheels/bd/22/e5/b69726d5e1a19795ecd3b3e7464b16c0f1d019aa94ff1c8578\n","Successfully built emoji\n","Installing collected packages: emoji, tqdm, scipy, python-crfsuite, ftfy, sklearn-crfsuite, gensim, bnlp_toolkit\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.67.1\n","    Uninstalling tqdm-4.67.1:\n","      Successfully uninstalled tqdm-4.67.1\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.13.1\n","    Uninstalling scipy-1.13.1:\n","      Successfully uninstalled scipy-1.13.1\n","  Attempting uninstall: gensim\n","    Found existing installation: gensim 4.3.3\n","    Uninstalling gensim-4.3.3:\n","      Successfully uninstalled gensim-4.3.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","scikit-image 0.25.0 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed bnlp_toolkit-4.0.3 emoji-1.7.0 ftfy-6.2.0 gensim-4.3.2 python-crfsuite-0.9.11 scipy-1.10.1 sklearn-crfsuite-0.3.6 tqdm-4.66.3\n"]}]},{"source":["from bnlp import BengaliWord2Vec # This line should work after installation\n","import gensim\n","from gensim.models import Word2Vec\n","\n","# ... (Rest of the code remains the same)"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":598},"id":"YeJnDGb6vkXP","executionInfo":{"status":"error","timestamp":1738064946115,"user_tz":-360,"elapsed":4333,"user":{"displayName":"Ak Rashed","userId":"12448226442715938476"}},"outputId":"9baaed3a-ba00-45ba-e86e-c97321fa83cf"},"id":"YeJnDGb6vkXP","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["punkt not found. downloading...\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.11/dist-packages/scipy/linalg/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-992b87b11bfa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbnlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBengaliWord2Vec\u001b[0m \u001b[0;31m# This line should work after installation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ... (Rest of the code remains the same)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bnlp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m from bnlp.embedding.word2vec import (\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mBengaliWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mWord2VecTraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bnlp/embedding/word2vec.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNLTKTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_blas_funcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlapack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpsi\u001b[0m  \u001b[0;31m# gamma function utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.11/dist-packages/scipy/linalg/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"source":["!pip install --upgrade scipy gensim bnlp_toolkit"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yPpLA3dwvtaP","executionInfo":{"status":"ok","timestamp":1738064983054,"user_tz":-360,"elapsed":4662,"user":{"displayName":"Ak Rashed","userId":"12448226442715938476"}},"outputId":"18ea196c-9c3b-4170-ed95-1c6af3a35530"},"id":"yPpLA3dwvtaP","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.10.1)\n","Collecting scipy\n","  Downloading scipy-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.2)\n","Collecting gensim\n","  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Requirement already satisfied: bnlp_toolkit in /usr/local/lib/python3.11/dist-packages (4.0.3)\n","Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (1.26.4)\n","Collecting scipy\n","  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.11/dist-packages (from bnlp_toolkit) (0.2.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from bnlp_toolkit) (3.9.1)\n","Requirement already satisfied: sklearn-crfsuite==0.3.6 in /usr/local/lib/python3.11/dist-packages (from bnlp_toolkit) (0.3.6)\n","Requirement already satisfied: tqdm==4.66.3 in /usr/local/lib/python3.11/dist-packages (from bnlp_toolkit) (4.66.3)\n","Requirement already satisfied: ftfy==6.2.0 in /usr/local/lib/python3.11/dist-packages (from bnlp_toolkit) (6.2.0)\n","Requirement already satisfied: emoji==1.7.0 in /usr/local/lib/python3.11/dist-packages (from bnlp_toolkit) (1.7.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bnlp_toolkit) (2.32.3)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.11/dist-packages (from ftfy==6.2.0->bnlp_toolkit) (0.2.13)\n","Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite==0.3.6->bnlp_toolkit) (0.9.11)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite==0.3.6->bnlp_toolkit) (1.17.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite==0.3.6->bnlp_toolkit) (0.9.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->bnlp_toolkit) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->bnlp_toolkit) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->bnlp_toolkit) (2024.11.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bnlp_toolkit) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bnlp_toolkit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bnlp_toolkit) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bnlp_toolkit) (2024.12.14)\n"]}]},{"cell_type":"code","execution_count":14,"id":"c9b429c0","metadata":{"id":"c9b429c0","colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"status":"error","timestamp":1738064991900,"user_tz":-360,"elapsed":536,"user":{"displayName":"Ak Rashed","userId":"12448226442715938476"}},"outputId":"82d15c61-3e60-48bc-b5cd-8dcee72416f5"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'w2v_model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-3148bc74acd8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./word_embeddings/fakeNews_word2vec_embeddings.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'w2v_model' is not defined"]}],"source":["filename = './word_embeddings/fakeNews_word2vec_embeddings.txt'\n","w2v_model.wv.save_word2vec_format(filename, binary=False)"]},{"cell_type":"markdown","id":"ffd2f51d","metadata":{"id":"ffd2f51d"},"source":["# Data processing for models and embedding matrix"]},{"cell_type":"code","execution_count":null,"id":"15e42e09","metadata":{"id":"15e42e09","outputId":"7bc173cd-5c33-4a72-e679-a12bc10aceec"},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of vocabulary :  52234\n","All sentances with same length  (2000, 300)\n","[0 1]\n","[[0. 1.]\n"," [0. 1.]\n"," [1. 0.]\n"," ...\n"," [1. 0.]\n"," [0. 1.]\n"," [1. 0.]]\n"]}],"source":["from sklearn.model_selection import train_test_split\n","from keras.utils import np_utils\n","import os\n","\n","# tokenize every words so that evey words maps to numaric value\n","tok = Tokenizer()\n","tok.fit_on_texts(data['content'].values.astype('U'))\n","encd_rev = tok.texts_to_sequences(data['content'].values.astype('U'))\n","# tok.word_index\n","\n","# make all the input sentance same length with add padding\n","max_rev_len = 40 # max lenght of a sentance\n","vocab_size = len(tok.word_index) + 1  # total no of words\n","embed_dim = 100 # embedding dimension\n","pad_rev= pad_sequences(encd_rev, maxlen=, padding='post')\n","\n","print(\"Size of vocabulary : \", vocab_size)\n","print(\"All sentances with same length \", pad_rev.shape)\n","\n","# lebel encode the output label to categorical\n","Y = data['label']\n","# encode class values as integers\n","encoder = LabelEncoder()\n","encoder.fit(Y)\n","encoded_Y = encoder.transform(Y)\n","# convert integers to dummy variables (i.e. one hot encoded)\n","dummy_y = np_utils.to_categorical(encoded_Y)\n","\n","print(encoder.classes_)\n","print(dummy_y)\n","\n","# test train split\n","X_train_word,X_test_word,y_train_word,y_test_word=train_test_split(pad_rev,dummy_y,test_size=0.30, random_state = 0)\n","\n","\n","# make a dictionary. word as key and feature vector as value\n","embedding_index={}\n","f = open('./word_embeddings/fakeNews_word2vec_embeddings.txt',encoding='utf-8')\n","for line in f:\n","    values=line.split()\n","    word=values[0]\n","    coefs=np.asarray(values[1:])\n","    embedding_index[word]=coefs\n","f.close()\n","\n","# create a embeddings matrix with 100 dimenstion\n","EMBEDDING_DIM=100\n","embedding_matrix=np.zeros((vocab_size,EMBEDDING_DIM))\n","for word, i in tok.word_index.items():\n","    if i>vocab_size:\n","        continue\n","    embedding_vector=embedding_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i]=embedding_vector"]},{"cell_type":"markdown","id":"ba6d4f15","metadata":{"id":"ba6d4f15"},"source":["# Model Training and Result evaluation helper function"]},{"cell_type":"code","execution_count":null,"id":"381da07b","metadata":{"id":"381da07b"},"outputs":[],"source":["from sklearn.metrics import classification_report, confusion_matrix\n","def print_result(model, history, x_test, y_test, name):\n","    model_path = \"./Models/\"+ name+ \" model\"\n","\n","    # convert the history.history dict to a pandas DataFrame:\n","    hist_df = pd.DataFrame(history.history)\n","\n","    # save to json:\n","    hist_json_file = \"./History/\" + name + 'history.json'\n","    with open(hist_json_file, mode='w') as f:\n","        hist_df.to_json(f)\n","\n","    #save model\n","    model.save(model_path)\n","\n","\n","    #ploting training  history\n","    accr = model.evaluate(x_test,y_test)\n","    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","\n","    y_pred = model.predict(x_test)\n","\n","    #normalize\n","    y_pred_frac=np.argmax(y_pred, axis=1)\n","    y_test_frac=np.argmax(y_test, axis=1)\n","\n","    print(\"-------------Classification Report----------------\")\n","    print(classification_report(y_test_frac, y_pred_frac))\n","    print(\"-------------------//*//-------------------------\")\n","\n","    print(\"-------------Confusion Matrix----------------\")\n","    cm = confusion_matrix(y_test_frac, y_pred_frac)\n","    print(cm)\n","    print(\"-------------------//*//-------------------------\")\n","\n","\n","\n","def train_model(model, X_train, y_train):\n","    epochs = 10\n","    batch_size = 64\n","    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n","    return history, model"]},{"cell_type":"markdown","id":"3b2f7c6e","metadata":{"id":"3b2f7c6e"},"source":["# Bi-LSTM"]},{"cell_type":"code","execution_count":null,"id":"d5c1271a","metadata":{"id":"d5c1271a","outputId":"f1bffcbf-17d2-486c-c45f-604f93715178"},"outputs":[{"name":"stdout","output_type":"stream","text":["summary of the built model..\n","Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_4 (Embedding)     (None, 300, 100)          5223400   \n","                                                                 \n"," bidirectional_4 (Bidirectio  (None, 256)              234496    \n"," nal)                                                            \n","                                                                 \n"," dense_4 (Dense)             (None, 2)                 514       \n","                                                                 \n","=================================================================\n","Total params: 5,458,410\n","Trainable params: 235,010\n","Non-trainable params: 5,223,400\n","_________________________________________________________________\n"]}],"source":["model=Sequential()\n","embedding_layer=Embedding(vocab_size,\n","                        EMBEDDING_DIM,\n","                        embeddings_initializer=Constant(embedding_matrix),\n","                        input_length=300,\n","                        trainable=False)\n","model.add(embedding_layer)\n","model.add(Bidirectional(LSTM(units=128,dropout=0.2,recurrent_dropout=0.2)))\n","model.add(Dense(2,activation='sigmoid'))\n","model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","print('summary of the built model..')\n","model.summary()"]},{"cell_type":"code","execution_count":null,"id":"43ca65a1","metadata":{"id":"43ca65a1","outputId":"6d1b37d8-52bb-4fde-8fad-1900676c06c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","20/20 [==============================] - 127s 6s/step - loss: 0.5390 - accuracy: 0.7325 - val_loss: 0.5069 - val_accuracy: 0.7714\n","Epoch 2/10\n","20/20 [==============================] - 124s 6s/step - loss: 0.3478 - accuracy: 0.8571 - val_loss: 0.3765 - val_accuracy: 0.8500\n","Epoch 3/10\n","20/20 [==============================] - 127s 6s/step - loss: 0.2611 - accuracy: 0.8944 - val_loss: 0.4659 - val_accuracy: 0.8214\n","Epoch 4/10\n","20/20 [==============================] - 128s 6s/step - loss: 0.2303 - accuracy: 0.9008 - val_loss: 0.4855 - val_accuracy: 0.8071\n","Epoch 5/10\n","20/20 [==============================] - 129s 6s/step - loss: 0.1876 - accuracy: 0.9246 - val_loss: 0.5113 - val_accuracy: 0.8286\n","Epoch 6/10\n","20/20 [==============================] - 129s 6s/step - loss: 0.1449 - accuracy: 0.9516 - val_loss: 0.5054 - val_accuracy: 0.8214\n","Epoch 7/10\n","20/20 [==============================] - 130s 7s/step - loss: 0.1085 - accuracy: 0.9643 - val_loss: 0.5051 - val_accuracy: 0.8143\n","Epoch 8/10\n","20/20 [==============================] - 132s 7s/step - loss: 0.0922 - accuracy: 0.9722 - val_loss: 0.5177 - val_accuracy: 0.8214\n","Epoch 9/10\n","20/20 [==============================] - 151s 8s/step - loss: 0.0849 - accuracy: 0.9722 - val_loss: 0.4908 - val_accuracy: 0.8643\n","Epoch 10/10\n","20/20 [==============================] - 153s 8s/step - loss: 0.0856 - accuracy: 0.9690 - val_loss: 0.4477 - val_accuracy: 0.8286\n"]}],"source":["history, model = train_model(model, X_train_word, y_train_word)"]},{"cell_type":"code","execution_count":null,"id":"6a5adfaa","metadata":{"id":"6a5adfaa","outputId":"8e8baa03-be82-4778-afac-d9123549493a"},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: ./Models/LSTM model\\assets\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000138471375B0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000013847137A00> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"]},{"name":"stdout","output_type":"stream","text":["19/19 [==============================] - 5s 220ms/step - loss: 0.6689 - accuracy: 0.6050\n","Test set\n","  Loss: 0.669\n","  Accuracy: 0.605\n","-------------Classification Report----------------\n","              precision    recall  f1-score   support\n","\n","           0       0.60      0.67      0.63       306\n","           1       0.61      0.54      0.57       294\n","\n","    accuracy                           0.60       600\n","   macro avg       0.61      0.60      0.60       600\n","weighted avg       0.61      0.60      0.60       600\n","\n","-------------------//*//-------------------------\n","-------------Confusion Matrix----------------\n","[[204 102]\n"," [135 159]]\n","-------------------//*//-------------------------\n"]}],"source":["print_result(model, history, X_test_word, y_test_word, 'LSTM')"]},{"cell_type":"markdown","id":"8effc5b5","metadata":{"id":"8effc5b5"},"source":["# Bi-GRU"]},{"cell_type":"code","execution_count":null,"id":"2b6f0466","metadata":{"id":"2b6f0466","outputId":"3fd89ff3-e006-41ee-aab3-744ff6a5c6b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["summary of the built model..\n","Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_6 (Embedding)     (None, 300, 100)          5223400   \n","                                                                 \n"," bidirectional_6 (Bidirectio  (None, 256)              176640    \n"," nal)                                                            \n","                                                                 \n"," dense_6 (Dense)             (None, 2)                 514       \n","                                                                 \n","=================================================================\n","Total params: 5,400,554\n","Trainable params: 177,154\n","Non-trainable params: 5,223,400\n","_________________________________________________________________\n"]}],"source":["model=Sequential()\n","embedding_layer=Embedding(vocab_size,\n","                        EMBEDDING_DIM,\n","                        embeddings_initializer=Constant(embedding_matrix),\n","                        input_length=300,\n","                        trainable=False)\n","model.add(embedding_layer)\n","model.add(Bidirectional(GRU(units=128,dropout=0.2,recurrent_dropout=0.2)))\n","model.add(Dense(2,activation='sigmoid'))\n","model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","print('summary of the built model..')\n","model.summary()"]},{"cell_type":"code","execution_count":null,"id":"1c8b6575","metadata":{"id":"1c8b6575","outputId":"b5ce7f19-f721-42a7-8cc8-fd2f65026cd3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","20/20 [==============================] - 106s 5s/step - loss: 0.6203 - accuracy: 0.6579 - val_loss: 0.6021 - val_accuracy: 0.7214\n","Epoch 2/10\n","20/20 [==============================] - 97s 5s/step - loss: 0.4682 - accuracy: 0.7825 - val_loss: 0.5410 - val_accuracy: 0.7429\n","Epoch 3/10\n","20/20 [==============================] - 91s 5s/step - loss: 0.3839 - accuracy: 0.8294 - val_loss: 0.5110 - val_accuracy: 0.7929\n","Epoch 4/10\n","20/20 [==============================] - 90s 4s/step - loss: 0.3219 - accuracy: 0.8587 - val_loss: 0.4965 - val_accuracy: 0.8000\n","Epoch 5/10\n","20/20 [==============================] - 91s 5s/step - loss: 0.2653 - accuracy: 0.8937 - val_loss: 0.5156 - val_accuracy: 0.7643\n","Epoch 6/10\n","20/20 [==============================] - 92s 5s/step - loss: 0.2170 - accuracy: 0.9190 - val_loss: 0.4924 - val_accuracy: 0.8071\n","Epoch 7/10\n","20/20 [==============================] - 90s 5s/step - loss: 0.1763 - accuracy: 0.9429 - val_loss: 0.5146 - val_accuracy: 0.7929\n","Epoch 8/10\n","20/20 [==============================] - 91s 5s/step - loss: 0.1578 - accuracy: 0.9492 - val_loss: 0.4788 - val_accuracy: 0.8143\n","Epoch 9/10\n","20/20 [==============================] - 91s 5s/step - loss: 0.1260 - accuracy: 0.9587 - val_loss: 0.5313 - val_accuracy: 0.8143\n","Epoch 10/10\n","20/20 [==============================] - 90s 5s/step - loss: 0.1101 - accuracy: 0.9619 - val_loss: 0.5857 - val_accuracy: 0.7714\n"]}],"source":["history, model = train_model(model, X_train_word, y_train_word)"]},{"cell_type":"code","execution_count":null,"id":"d3dad433","metadata":{"id":"d3dad433","outputId":"4e7719c1-a5ff-42ef-f9ae-c433602b1527"},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: ./Models/GRU model\\assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: ./Models/GRU model\\assets\n","WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000138726D19A0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000138726D1AC0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"]},{"name":"stdout","output_type":"stream","text":["19/19 [==============================] - 6s 340ms/step - loss: 0.4108 - accuracy: 0.8667\n","Test set\n","  Loss: 0.411\n","  Accuracy: 0.867\n","-------------Classification Report----------------\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.83      0.86       306\n","           1       0.84      0.90      0.87       294\n","\n","    accuracy                           0.87       600\n","   macro avg       0.87      0.87      0.87       600\n","weighted avg       0.87      0.87      0.87       600\n","\n","-------------------//*//-------------------------\n","-------------Confusion Matrix----------------\n","[[255  51]\n"," [ 29 265]]\n","-------------------//*//-------------------------\n"]}],"source":["print_result(model, history, X_test_word, y_test_word, 'GRU')"]}],"metadata":{"kernelspec":{"display_name":"py37","language":"python","name":"py37"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}